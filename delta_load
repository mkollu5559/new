import pytest
import sys
import os
from unittest.mock import MagicMock, patch
from pyspark.sql import SparkSession
from pyspark.sql.types import *


dbutils_mock = MagicMock()
dbutils_mock.widgets.get.side_effect = lambda k: {
    'Primary_Key': 'id',
    'Incr_TimeStamp_Column': 'updated_at',
    'Delta_Table_Name': 'test_table',
    'source_s3_path': 's3://dummy/source/',
    'Delta_Table_db': 'test_db',
    'table_config_file': 's3://dummy/config.txt',
    'refresh_type': 'incr',
    'env': 'dev',
    'incr_ingest_timestamp': '2025-04-07-00:00:00',
    'run_type': 'adhoc',
    'table_refresh_type': 'incr',
    'source_type': 's3',
    'number_of_files': '1000'
}.get(k, '')

sys.modules['dbutils'] = dbutils_mock

# -------------------------------------------
# STEP 2: Mock delta.tables before importing source
# -------------------------------------------
sys.modules['delta'] = MagicMock()
sys.modules['delta.tables'] = MagicMock()

# -------------------------------------------
# STEP 3: Add root path to sys.path if needed
# -------------------------------------------
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

# -------------------------------------------
# STEP 4: Import the source (after mocks!)
# -------------------------------------------
from module.databricks.madatasync.artifacts.notebooks import maa_databricks_ntbk_madatasync_delta_load as script

# -------------------------------------------
# STEP 5: Spark Session Fixture
# -------------------------------------------
@pytest.fixture(scope="session")
def spark_session():
    return SparkSession.builder.master("local[*]").appName("TestSession").getOrCreate()

# -------------------------------------------
# Example test
# -------------------------------------------
def test_get_data_type_decimal():
    dtype = script.get_data_type("decimal(10,2)")
    assert isinstance(dtype, DecimalType)
    assert dtype.precision == 10 and dtype.scale == 2

def test_parse_schema_valid():
    source = [{"col_name": "id", "data_type": "int"}]
    target = [{"col_name": "id", "data_type": "string"}]
    parsed = script.parse_schema(source, target)
    assert parsed == [{"col_name": "id", "src_type": "int", "tgt_type": "string"}]

def test_get_spark_schema_valid():
    details = [{"col_name": "id", "src_type": "STRING", "tgt_type": "INT"}]
    src_schema, tgt_schema = script.get_spark_schema(details)
    assert isinstance(src_schema, StructType)
    assert "id" in tgt_schema
    assert isinstance(tgt_schema["id"], IntegerType)

def test_apply_mapping_casts_column(spark_session):
    df = spark_session.createDataFrame([("123",)], ["id"])
    schema_details = [{"col_name": "id", "src_type": "STRING", "tgt_type": "INT"}]
    result_df = script.ApplyMapping(df, schema_details)
    assert result_df.schema["id"].dataType == IntegerType()
