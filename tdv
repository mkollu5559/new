import sys
import json
import datetime
import pandas as pd
import pytest
from unittest.mock import MagicMock

import teradatasql

# Import your module and class.
# Adjust the import if your file or package name is different.
from db_config_create import Db_Config_Create, s3_resource

# A fake set of resolved arguments to simulate the behavior of awsglue.utils.getResolvedOptions.
fake_args = {
    "OSS_ENV": "dev",
    "DATABASE_NAME": "TEST_DB",    # Use a name that does NOT end with '_V' for one branch.
    "TABLE_NAME": "TEST_TABLE",
    "SERVER": "test.server.com",
    "secret_name": "test_secret",
    "BUCKET_NAME": "test-bucket",
    "JOB_RUN_ID": "1234",
}


@pytest.fixture(autouse=True)
def set_fake_args(monkeypatch):
    """
    Override the global args in the module so that the class is initialized with predictable values.
    """
    monkeypatch.setattr("db_config_create.args", fake_args)


def test_get_tdv_creds_success(monkeypatch):
    """
    Test that get_tdv_creds returns the correct credentials when a valid secret is returned.
    """
    fake_secret = {"username": "fake_user", "password": "fake_password"}
    fake_get_secret_value_response = {"SecretString": json.dumps(fake_secret)}

    fake_client = MagicMock()
    fake_client.get_secret_value.return_value = fake_get_secret_value_response

    fake_session = MagicMock()
    fake_session.client.return_value = fake_client

    # Patch the boto3 session so that our fake session is used.
    monkeypatch.setattr("boto3.session.Session", lambda: fake_session)

    db_config = Db_Config_Create()
    username, password = db_config.get_tdv_creds()

    assert username == "fake_user"
    assert password == "fake_password"


def test_get_tdv_creds_failure(monkeypatch):
    """
    Test that get_tdv_creds raises an exception when the secret cannot be retrieved.
    """
    fake_client = MagicMock()
    fake_client.get_secret_value.side_effect = Exception("Secret not found")

    fake_session = MagicMock()
    fake_session.client.return_value = fake_client

    monkeypatch.setattr("boto3.session.Session", lambda: fake_session)

    db_config = Db_Config_Create()

    with pytest.raises(Exception, match="Secret not found"):
        db_config.get_tdv_creds()


def test_func_tdconnect(monkeypatch):
    """
    Test that func_tdconnect calls get_tdv_creds and returns a Teradata connection.
    """
    # Patch get_tdv_creds to return fixed credentials.
    monkeypatch.setattr(
        Db_Config_Create, "get_tdv_creds", lambda self: ("fake_user", "fake_password")
    )

    # Create a fake connection and capture the parameters passed to teradatasql.connect.
    fake_connection = MagicMock()
    captured_params = {}

    def fake_tdconnect(**kwargs):
        captured_params.update(kwargs)
        return fake_connection

    monkeypatch.setattr(teradatasql, "connect", fake_tdconnect)

    db_config = Db_Config_Create()
    connection = db_config.func_tdconnect()

    assert connection == fake_connection
    assert captured_params["host"] == fake_args["SERVER"]
    assert captured_params["user"] == "fake_user"
    assert captured_params["password"] == "fake_password"
    # Optionally, you can also check for the presence of other parameters like LOGMECH and encryptdata.


def test_update_audit_table(monkeypatch):
    """
    Test that update_audit_table calls the audit table's update_item method with the expected values.
    """
    db_config = Db_Config_Create()
    fake_audit_table = MagicMock()
    db_config.audit_table = fake_audit_table

    run_id = fake_args["JOB_RUN_ID"]
    source_db = fake_args["DATABASE_NAME"]
    source_table = fake_args["TABLE_NAME"]
    run_status = "Success"
    error_message = ""

    db_config.update_audit_table(run_id, source_db, source_table, run_status, error_message)
    fake_audit_table.update_item.assert_called_once()
    # You may inspect fake_audit_table.update_item.call_args for more detailed parameter checks.


def test_write_to_s3_else_branch(monkeypatch):
    """
    Test write_to_s3 when the DATABASE_NAME does NOT end with '_V'.
    This simulates the non-view branch.
    """
    # Create a fake Spark DataFrame using a simple pandas DataFrame.
    fake_pdf = pd.DataFrame({
        "ColumnId": [1, 2],
        "COL_DATATYPE": ["col1 INT", "col2 VARCHAR"]
    })

    # Define fake classes to simulate Spark's DataFrame and DataFrameReader.
    class FakeDataFrame:
        def __init__(self, pdf):
            self.pdf = pdf

        def count(self):
            return len(self.pdf)

        def toPandas(self):
            return self.pdf

    class FakeDataFrameReader:
        def __init__(self, pdf):
            self.pdf = pdf

        def format(self, fmt):
            return self

        def option(self, key, value):
            return self

        def load(self):
            return FakeDataFrame(fake_pdf)

    class FakeSparkSession:
        def __init__(self, pdf):
            self.read = FakeDataFrameReader(pdf)

    db_config = Db_Config_Create()
    db_config.spark = FakeSparkSession(fake_pdf)

    # Patch get_tdv_creds to return fake credentials.
    monkeypatch.setattr(db_config, "get_tdv_creds", lambda: ("fake_user", "fake_password"))

    # Patch update_audit_table so we can verify it gets called.
    audit_called = False

    def fake_update_audit_table(*args, **kwargs):
        nonlocal audit_called
        audit_called = True

    monkeypatch.setattr(db_config, "update_audit_table", fake_update_audit_table)

    # Create a fake S3 storage to capture the files written.
    fake_s3_storage = {}

    def fake_s3_object(bucket, key):
        class FakeS3Object:
            def put(self, Body):
                fake_s3_storage[key] = Body
        return FakeS3Object()

    monkeypatch.setattr(s3_resource, "Object", fake_s3_object)

    # Patch teradatasql.connect to return a dummy connection (not used in this branch).
    monkeypatch.setattr(teradatasql, "connect", lambda **kwargs: MagicMock())

    # Execute the write_to_s3 method.
    db_config.write_to_s3()

    # Construct the expected S3 file paths.
    config_file_name_txt = f"{db_config.db_name.lower()}_{db_config.table_name.lower()}.txt"
    config_file_name_json = f"{db_config.db_name.lower()}_{db_config.table_name.lower()}.json"
    s3_file_path_txt = f"{db_config.bucket_key}{config_file_name_txt}"
    s3_file_path_json = f"{db_config.bucket_key}{config_file_name_json}"

    # Verify the TXT file content.
    expected_txt = "col1 INT,col2 VARCHAR"
    assert fake_s3_storage[s3_file_path_txt] == expected_txt

    # Verify the JSON file content.
    expected_json = {"col1": "INT", "col2": "VARCHAR"}
    actual_json = json.loads(fake_s3_storage[s3_file_path_json].decode("utf-8"))
    assert actual_json == expected_json

    # Confirm that update_audit_table was called.
    assert audit_called


def test_write_to_s3_view_branch(monkeypatch):
    """
    Test write_to_s3 when the DATABASE_NAME ends with '_V' (the view branch).
