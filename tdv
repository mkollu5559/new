import sys
import json
import base64
import pytest
import pandas as pd
from datetime import datetime
from unittest.mock import MagicMock

# Before importing the module under test, override getResolvedOptions so that
# the module-level call doesn’t fail.
dummy_args = {
    "OSS_ENV": "dev",
    "DATABASE_NAME": "TEST_DB",
    "TABLE_NAME": "TEST_TABLE",
    "SERVER": "test_server",
    "secret_name": "test_secret",
    "BUCKET_NAME": "test_bucket",
    "JOB_RUN_ID": "test_job_run_id"
}

# Override the glue utility to always return our dummy_args.
import awsglue.utils
awsglue.utils.getResolvedOptions = lambda argv, keys: dummy_args

# Now import the module under test.
import db_config_create


# --- Fake classes for Spark and S3 ---

class FakeDataFrame:
    """A fake Spark DataFrame for testing purposes."""
    def __init__(self, count_value, pandas_df):
        self._count_value = count_value
        self._pandas_df = pandas_df

    def count(self):
        return self._count_value

    def toPandas(self):
        return self._pandas_df


class FakeSparkRead:
    """A fake chain for spark.read.format().option()...load()."""
    def __init__(self, fake_df):
        self.fake_df = fake_df

    def format(self, fmt):
        return self

    def option(self, key, value):
        return self

    def load(self):
        return self.fake_df


class FakeS3Object:
    """A fake S3 object to record put calls."""
    def __init__(self):
        self.body = None

    def put(self, Body):
        self.body = Body
        return {"ResponseMetadata": {"HTTPStatusCode": 200}}


class FakeS3Resource:
    """A fake S3 resource that tracks written objects in a dict."""
    def __init__(self):
        self.objects = {}

    def Object(self, bucket_name, key):
        fake_obj = FakeS3Object()
        self.objects[(bucket_name, key)] = fake_obj
        return fake_obj


# --- Pytest fixtures ---

@pytest.fixture
def db_config_instance(monkeypatch):
    """
    Create an instance of Db_Config_Create with some dependencies overridden.
    We override:
      - get_tdv_creds to return dummy credentials.
      - update_audit_table to record calls in an attribute.
      - func_tdconnect (if needed).
    """
    instance = db_config_create.Db_Config_Create()
    instance.get_tdv_creds = lambda: ("test_user", "test_pass")
    instance.func_tdconnect = lambda: MagicMock()

    # Record audit updates in a list for verification.
    instance.audit_updates = []
    def fake_update_audit_table(Run_ID, Source_DatabaseName, Source_TableName, Run_Status, Error_Message):
        instance.audit_updates.append(
            (Run_ID, Source_DatabaseName, Source_TableName, Run_Status, Error_Message)
        )
    instance.update_audit_table = fake_update_audit_table

    return instance


@pytest.fixture
def fake_s3_resource():
    """Provide a fake S3 resource instance."""
    return FakeS3Resource()


# --- Test cases ---

def test_get_tdv_creds(monkeypatch, db_config_instance):
    """
    Test that get_tdv_creds returns credentials extracted from the secret.
    We simulate a Secrets Manager client returning a JSON string.
    """
    fake_response = {
        'SecretString': json.dumps({"username": "fake_user", "password": "fake_pass"})
    }
    fake_client = MagicMock()
    fake_client.get_secret_value.return_value = fake_response

    fake_session = MagicMock()
    fake_session.client.return_value = fake_client

    # Patch boto3.session.Session so that our fake_session is returned.
    monkeypatch.setattr(db_config_create.boto3.session, "Session", lambda: fake_session)

    username, password = db_config_instance.get_tdv_creds()
    assert username == "fake_user"
    assert password == "fake_pass"


def test_func_tdconnect(monkeypatch, db_config_instance):
    """
    Test that func_tdconnect calls get_tdv_creds and uses teradatasql.connect.
    """
    fake_conn = MagicMock()

    def fake_tdconnect(*args, **kwargs):
        return fake_conn

    monkeypatch.setattr(db_config_create.teradatasql, "connect", fake_tdconnect)
    # Ensure get_tdv_creds returns known credentials.
    db_config_instance.get_tdv_creds = lambda: ("user1", "pass1")
    conn = db_config_instance.func_tdconnect()
    assert conn is fake_conn


def test_update_audit_table(monkeypatch, db_config_instance):
    """
    Test that update_audit_table calls the audit table's update_item method with expected parameters.
    """
    fake_audit_table = MagicMock()
    db_config_instance.audit_table = fake_audit_table

    db_config_instance.update_audit_table("test_run", "TEST_DB", "TEST_TABLE", "Success", "")
    # Check that update_item was called and that the Run_Status value is "Success".
    args, kwargs = fake_audit_table.update_item.call_args
    assert "ExpressionAttributeValues" in kwargs
    assert kwargs["ExpressionAttributeValues"][":Run_Status"] == "Success"


def test_write_to_s3_normal(monkeypatch, db_config_instance, fake_s3_resource):
    """
    Test the write_to_s3 method when db_name does NOT end with '_V' (the normal branch).
    We simulate the Spark DataFrame returned by spark.read and override s3_resource.
    """
    # Force a db_name that does not trigger the temp table branch.
    db_config_instance.db_name = "TESTDB"
    monkeypatch.setattr(db_config_create, "s3_resource", fake_s3_resource)

    # Create a dummy pandas DataFrame to simulate the JDBC query result.
    dummy_pdf = pd.DataFrame({
        "ColumnId": [1],
        "COL_DATATYPE": ["col1 INT"]
    })
    fake_df = FakeDataFrame(count_value=1, pandas_df=dummy_pdf)
    fake_read = FakeSparkRead(fake_df)
    db_config_instance.spark = MagicMock()
    db_config_instance.spark.read = fake_read

    # Override teradatasql.connect so that no real connection is made.
    monkeypatch.setattr(db_config_create.teradatasql, "connect", lambda *args, **kwargs: MagicMock())

    # Call write_to_s3; it should follow the normal branch.
    db_config_instance.write_to_s3()

    # Construct expected S3 keys.
    config_file_name_txt = f"{db_config_instance.db_name.lower()}_{db_config_instance.table_name.lower()}.txt"
    config_file_name_json = f"{db_config_instance.db_name.lower()}_{db_config_instance.table_name.lower()}.json"
    s3_key_txt = f"{db_config_instance.bucket_key}{config_file_name_txt}"
    s3_key_json = f"{db_config_instance.bucket_key}{config_file_name_json}"

    # Verify that the TXT and JSON objects were “put” in our fake S3.
    assert (db_config_instance.bucket_name, s3_key_txt) in fake_s3_resource.objects
    assert (db_config_instance.bucket_name, s3_key_json) in fake_s3_resource.objects

    # TXT file should have the joined column datatype string.
    txt_obj = fake_s3_resource.objects[(db_config_instance.bucket_name, s3_key_txt)]
    assert txt_obj.body == "col1 INT"

    # JSON file should decode to {"col1": "INT"}.
    json_obj = fake_s3_resource.objects[(db_config_instance.bucket_name, s3_key_json)]
    json_content = json.loads(json_obj.body.decode("UTF-8"))
    assert json_content == {"col1": "INT"}

    # Verify that update_audit_table was called (our fake records calls in audit_updates).
    assert len(db_config_instance.audit_updates) > 0
    last_update = db_config_instance.audit_updates[-1]
    assert last_update[3] == "Success"


def test_write_to_s3_with_temp_table(monkeypatch, db_config_instance, fake_s3_resource):
    """
    Test the write_to_s3 branch for when db_name ends with '_V' (the temp table branch).
    We simulate two separate Spark DataFrame loads:
      - The first call (check_temp_tbl_df) returns a count > 0 (forcing a drop of an existing temp table).
      - The second call (config_temp_df) returns a dummy configuration result.
    """
    # Set db_name so that the last two characters are '_V'.
    db_config_instance.db_name = "TESTDB_V"
    monkeypatch.setattr(db_config_create, "s3_resource", fake_s3_resource)

    # Simulate the two DataFrame calls.
    fake_df_check = FakeDataFrame(count_value=1, pandas_df=pd.DataFrame({"dummy": [1]}))
    dummy_pdf = pd.DataFrame({
        "ColumnId": [1],
        "COL_DATATYPE": ["col1 INT"]
    })
    fake_df_config = FakeDataFrame(count_value=1, pandas_df=dummy_pdf)

    # A fake reader that returns different fake dataframes on consecutive calls.
    class FakeSparkReadTemp:
        def __init__(self):
            self.call_count = 0

        def format(self, fmt):
            return self

        def option(self, key, value):
            return self

        def load(self):
            self.call_count += 1
            # First call returns check_temp_tbl_df; subsequent call returns config_temp_df.
            if self.call_count == 1:
                return fake_df_check
            else:
                return fake_df_config

    fake_read = FakeSparkReadTemp()
    db_config_instance.spark = MagicMock()
    db_config_instance.spark.read = fake_read

    # Create a fake cursor and connection for teradatasql.
    fake_cursor = MagicMock()
    fake_cursor.execute = MagicMock()
    fake_conn = MagicMock()
    fake_conn.cursor.return_value = fake_cursor
    monkeypatch.setattr(db_config_create.teradatasql, "connect", lambda *args, **kwargs: fake_conn)

    # Call write_to_s3; it should follow the branch for temp table creation.
    db_config_instance.write_to_s3()

    # Verify that SQL statements for dropping and creating the temp table were executed.
    calls = fake_cursor.execute.call_args_list
    assert any("DROP TABLE" in call.args[0] for call in calls)
    assert any("CREATE TABLE" in call.args[0] for call in calls)

    # Construct expected S3 keys.
    config_file_name_txt = f"{db_config_instance.db_name.lower()}_{db_config_instance.table_name.lower()}.txt"
    config_file_name_json = f"{db_config_instance.db_name.lower()}_{db_config_instance.table_name.lower()}.json"
    s3_key_txt = f"{db_config_instance.bucket_key}{config_file_name_txt}"
    s3_key_json = f"{db_config_instance.bucket_key}{config_file_name_json}"

    # Verify that the TXT and JSON objects were “put” in our fake S3.
    assert (db_config_instance.bucket_name, s3_key_txt) in fake_s3_resource.objects
    assert (db_config_instance.bucket_name, s3_key_json) in fake_s3_resource.objects

    txt_obj = fake_s3_resource.objects[(db_config_instance.bucket_name, s3_key_txt)]
    assert txt_obj.body == "col1 INT"

    json_obj = fake_s3_resource.objects[(db_config_instance.bucket_name, s3_key_json)]
    json_content = json.loads(json_obj.body.decode("UTF-8"))
    assert json_content == {"col1": "INT"}

    # Verify audit update call.
    assert len(db_config_instance.audit_updates) > 0
    last_update = db_config_instance.audit_updates[-1]
    assert last_update[3] == "Success"
