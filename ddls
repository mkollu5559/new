import pytest
from unittest.mock import patch, MagicMock
from databricks.connect import DatabricksSession


dbutils = MagicMock()
dbutils.widgets.get = lambda key: {
    "env": "dev",
    "domain_role": "data_engineer",
    "volume_name": "vol_test",
    "custom_sql": "",
    "delta_db": "test_db",
    "delta_tbl": "test_table",
    "catalog_name": "main"
}.get(key, "")

sys.modules['dbutils'] = dbutils

import create_volume_job as job

@pytest.fixture(scope="module")
def spark():
    return DatabricksSession.builder.getOrCreate()

@patch("create_volume_job.spark")
def test_create_volume_valid(mock_spark):
    msg = job.create_volume("dev", "vol1", "dev_role")
    mock_spark.sql.assert_called_once()
    assert "created successfully" in msg

def test_create_volume_invalid_env():
    msg = job.create_volume("stage", "vol1", "dev_role")
    assert "Invalid env" in msg

@patch("create_volume_job.spark")
@patch("create_volume_job.dbutils")
def test_execute_sql_valid(mock_dbutils, mock_spark):
    custom_sql = "SELECT * FROM dummy_table WHERE length(name) > 10 AND length(address) > 5 AND id > 100 AND score > 80"
    msg = job.execute_sql(custom_sql, "", "", "", "")
    mock_spark.sql.assert_called_once_with(custom_sql)
    assert "executed successfully" in msg

@patch("create_volume_job.dbutils")
def test_execute_sql_checkpoint(mock_dbutils):
    mock_dbutils.fs.rm.return_value = True
    msg = job.execute_sql("checkpoint", "mydb", "tbl", "catalog", "vol")
    assert "Checkpoint" in msg

def test_execute_sql_invalid_short():
    msg = job.execute_sql("select *", "", "", "", "")
    assert "Invalid SQL" in msg

def test_execute_sql_invalid_db_tbl():
    msg = job.execute_sql("checkpoint", "x", "y", "", "")
    assert "valid delta_db" in msg
