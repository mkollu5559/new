import sys
import os
import pytest
from unittest.mock import MagicMock
from databricks.connect import DatabricksSession

# --------------------
# MOCK dbutils
# --------------------
dbutils_mock = MagicMock()
dbutils_mock.widgets.get = lambda key: {
    "env": "dev",
    "domain_role": "data_engineer",
    "volume_name": "vol_test",
    "custom_sql": "",
    "delta_db": "test_db",
    "delta_tbl": "test_table",
    "catalog_name": "main"
}.get(key, "")
sys.modules["dbutils"] = dbutils_mock

# --------------------
# IMPORT notebook
# --------------------
sys.path.append(os.path.abspath("module/databricks/madatasync/artifacts/notebooks"))
import maa_databricks_ntbk_madatasync_ddls as job

# --------------------
# SPARK session fixture
# --------------------
@pytest.fixture(scope="module")
def spark():
    return DatabricksSession.builder.getOrCreate()

# --------------------
# TEST CASES
# --------------------
def test_create_volume_valid(spark):
    msg = job.create_volume("dev", "vol1", "dev_role")
    assert "created successfully" in msg

def test_create_volume_invalid_env(spark):
    msg = job.create_volume("stage", "vol1", "dev_role")
    assert "Invalid env" in msg
